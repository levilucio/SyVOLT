[%
var ruleIndex = 0;
var ruleShortMap : new Map;
var transformationArrayStr = "";
var lastLayerTreated = "";
var inputMM = "";
var outputMM = "";
%]

#-----------------------------------------------------------------------------
# Auto generated from the DSLTrans transformation and the properties to prove
#-----------------------------------------------------------------------------

import time

from path_condition_generator import PathConditionGenerator
from PyRamify import PyRamify

from ecore_utils import EcoreUtils
from core.himesis_plus import buildPreListFromClassNames

from property_prover_rules.HEmptyPathCondition import HEmptyPathCondition

from PropertyVerification.state_property import StateProperty
from PropertyVerification.atomic_state_property import AtomicStateProperty
from PropertyVerification.and_state_property import AndStateProperty
from PropertyVerification.or_state_property import OrStateProperty
from PropertyVerification.not_state_property import NotStateProperty
from PropertyVerification.implication_state_property import ImplicationStateProperty

from PropertyVerification.Not import Not 					#StateSpace Prop
from PropertyVerification.Implication import Implication 	#StateSpace Prop
from PropertyVerification.And import And 					#StateSpace Prop
from PropertyVerification.Or import Or 						#StateSpace Prop

from core.himesis_utils import graph_to_dot
from util.test_script_utils import select_rules, slice_transformation

# imports for properties' atomic contracts

[%
for (ac in inputProperties!AtomicContract) { %]
from H[%=ac.Name%]_IsolatedLHS import H[%=ac.Name%]_IsolatedLHS
from H[%=ac.Name%]_ConnectedLHS import H[%=ac.Name%]_ConnectedLHS   	
from H[%=ac.Name%]_CompleteLHS import H[%=ac.Name%]_CompleteLHS
[%}%]


class Prover():


    def do_proof(self,args):    

        pyramify = PyRamify(verbosity=args.verbosity, draw_svg=args.draw_svg)
        
        
        
        
        [*- get all the rules from the files generated to disk *]
        [% for (r in Rule.all) { %]
        r[%=ruleIndex%] = 'H[%=r.Description%]'
        [% ruleShortMap.put(r, ruleIndex);
        	ruleIndex = ruleIndex + 1;
           }
        %]    
        
        [*- build the transformation array with the right layering *]                     
        [% 
        for (l in Sequential) {
			for (prev in l.previousSource) {
				if (prev.isKindOf(FilePort)) {
        			transformationArrayStr = transformationArrayStr + "[";
        			for (r in l.hasRule) {
        			    transformationArrayStr = transformationArrayStr + "r" + ruleShortMap.get(r) + (",");
        			}
        			transformationArrayStr = transformationArrayStr + "],";
        	        lastLayerTreated = l.Name;
        	        inputMM = prev.metaModelId.metaModelURI;
        	        break;
        	     }
        	 }
        }
        
        var isLastLayer = false;  
        
        while (not isLastLayer) { 
			isLastLayer = true;
			for (l in Sequential) {
				for (prev in l.previousSource) {
					if (prev.isKindOf(Sequential) and prev.Name == lastLayerTreated) {
        				transformationArrayStr = transformationArrayStr + "[";
        				for (r in l.hasRule) {
        			    	transformationArrayStr = transformationArrayStr + "r" + ruleShortMap.get(r) + (",");
        				}
        				transformationArrayStr = transformationArrayStr + "],";
						lastLayerTreated = l.Name;
        	      		outputMM = l.metaModelId.metaModelURI;
                  		isLastLayer = false;       	   	  
        	      		break;
        	      	}
				}
        	 }
		} 
		%]
        full_transformation = [[%=transformationArrayStr%]]
        
        self.rules, self.transformation = pyramify.get_rules("[%=absoluteBackendPath%]", full_transformation)
        
        subclasses_dict, superclasses_dict = self.get_sub_and_super_classes()

        [self.rules, self.ruleTraceCheckers, backwardPatterns2Rules, backwardPatternsComplete, self.matchRulePatterns, self.ruleCombinators, self.overlapping_rules, self.subsumption, self.loopingRuleSubsumption] = \
            pyramify.ramify_directory("[%=absoluteBackendPath%]", self.transformation)   

        [*- Generate operations to deal with Polymorphism *]        
        pre_metamodel = ["MT_pre__S_MM", "MoTifRule"]
        post_metamodel = ["MT_post__T_MM", "MoTifRule"]

        pyramify.changePropertyProverMetamodel(pre_metamodel, post_metamodel, subclasses_dict, "[%=dsltransInstallPath%]")
        
        # go through all the matchers, combinators and tracers to add polymorphism on all classes in an inheritance hierarchy
                                                                  
        
        # add polymorphism for the matchers
        for matcher_key in self.matchRulePatterns.keys():
            self.matchRulePatterns[matcher_key][0].condition["superclasses_dict"] = superclasses_dict
            
        # add polymorphism for the combinators
        for combs_key in self.ruleCombinators.keys():
            if self.ruleCombinators[combs_key] != None:
                for combinator in self.ruleCombinators[combs_key]:
                    combinator[0].condition["superclasses_dict"] = superclasses_dict

        # add polymorphism for the tracers
        for tracer_key in self.ruleTraceCheckers.keys():
            if self.ruleTraceCheckers[tracer_key] != None:
                self.ruleTraceCheckers[tracer_key].condition["superclasses_dict"] = superclasses_dict    
            
            
        # load the contracts, and add polymorphism

        if (args.draw_svg): 
        [%
        for (ac in inputProperties!AtomicContract) { %]
        	graph_to_dot("property_[%=ac.Name%]_isolated", H[%=ac.Name%]_IsolatedLHS())
        	graph_to_dot("property_[%=ac.Name%]_connected", H[%=ac.Name%]_ConnectedLHS())
        	graph_to_dot("property_[%=ac.Name%]_complete", H[%=ac.Name%]_CompleteLHS())
        [%}%]       

        self.atomic_contracts = []
        
        [%
	    var atomicContractIndex = 0;
        for (ac in inputProperties!AtomicContract) { %]
        
        isolated = H[%=ac.Name%]_IsolatedLHS()
        connected = H[%=ac.Name%]_ConnectedLHS()
        complete = H[%=ac.Name%]_CompleteLHS()
        
        isolated["superclasses_dict"] = superclasses_dict 
        connected["superclasses_dict"] = superclasses_dict 
        complete["superclasses_dict"] = superclasses_dict 
        
        c[%=atomicContractIndex%] = AtomicStateProperty(isolated, connected, complete)
        
        self.atomic_contracts.append(("[%=ac.Name%]", c[%=atomicContractIndex%]))
        
          
        [%}%]
        
        if args.slice > 0:
            print("Slicing for contract number " + str(args.slice))
            contract = self.atomic_contracts[args.slice - 1]
            self.rules, self.transformation = slice_transformation(self.rules, self.transformation, contract, args)
            
            
		# generate path conditions
        pc_set = PathConditionGenerator(self.transformation, self.ruleCombinators, self.ruleTraceCheckers, self.matchRulePatterns, self.overlapping_rules, self.subsumption, self.loopingRuleSubsumption, args)
    
        ts0 = time.time()
        pc_set.build_path_conditions()
        ts1 = time.time()
             
        print("\n\nTime to build the set of path conditions: " + str(ts1 - ts0))
        print("Number of path conditions: " + str(pc_set.num_path_conditions))
 
        # print path conditions to screen
        
        if pc_set.num_path_conditions < 300:
            pc_set.print_path_conditions_screen()
        
        # now actually prove these contracts
        
        
        ts0 = time.time()
        
        verifier = StateProperty()
        
        for name, a_c in self.atomic_contracts:
            result = verifier.verifyCompositeStateProperty(pc_set, a_c)
            if len(result) == 0:
            	print("\n")
            	print("Contract " + name + " holds!")
            else:
            	print("\n")
            	print("Contract " + name + " does not hold!")
            	print(result)
        
        ts1 = time.time()
        
        print("\n\nTime to verify properties: " + str(ts1 - ts0) + " seconds.")

        
    def get_sub_and_super_classes(self):
            subclasses_dict = {}     
            
            inputMM = "[%=inputMM%]".split("/")[-1]
            outputMM = "[%=outputMM%]".split("/")[-1]
         
            inMM = EcoreUtils(inputMM)          
            subclasses_dict["MT_pre__MetaModelElement_S"] = buildPreListFromClassNames(inMM.getMetamodelClassNames())
            
            print(subclasses_dict["MT_pre__MetaModelElement_S"])

            outMM = EcoreUtils(outputMM)  
            subclasses_dict["MT_pre__MetaModelElement_T"] = buildPreListFromClassNames(outMM.getMetamodelClassNames()) 
            
            print(subclasses_dict["MT_pre__MetaModelElement_T"])

            # keep a dictionary from each child to its parent
            supertypes = {}

            for supertype in subclasses_dict:
                for subtype in subclasses_dict[supertype]:
                    subtype = subtype[8:]
                    try:
                        supertypes[subtype].append(supertype[8:])
                    except KeyError:
                        supertypes[subtype] = [supertype[8:]]

            return subclasses_dict, supertypes



if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='Run the ecore_copier test.')
    
    parser.add_argument('--skip_tests', dest = 'run_tests', action = 'store_false',
                        help = 'Option to skip the running of matching tests')
    parser.set_defaults(run_tests = True)

    parser.add_argument('--skip_parallel', dest = 'do_parallel', action = 'store_false',
                        help = 'Option to force computation to run single-thread')
    parser.set_defaults(do_parallel = True)

    parser.add_argument('--skip_pickle', dest = 'do_pickle', action = 'store_false',
                        help = 'Option to skip the use of pickling')
    parser.set_defaults(do_pickle = True)
    
    parser.add_argument('--compression', type = int, default = 6,
                        help = 'Level of compression to use with pickling. Range: 0 (no compression) to 9 (high compression) (default: 6)')
    parser.set_defaults(compression = 6)
    
    parser.add_argument('--slice', type = int, default = 0,
                        help = 'Index of contract to slice for. Range: 0 (no slicing) to #CONTRACTS (default: 0)')
    parser.set_defaults(slice = 0)
    
    parser.add_argument('--no_svg', dest = 'draw_svg', action = 'store_false',
                        help = 'Flag to force svg files to not be drawn')
    parser.set_defaults(draw_svg = True)

    parser.add_argument('--num_pcs', type = int, default = -1,
                        help = 'Number of path conditions which should be produced by this test (default: -1)')

    parser.add_argument('--num_rules', type = int, default = -1,
                        help = 'Number of rules in the transformation (default: -1)')
                        
    parser.add_argument('--verbosity', type = int, default = 0,
                        help = 'Verbosity level (default: 0 - minimum output)')
                        
    args = parser.parse_args()


    prover = Prover()
    prover.do_proof(args)
        
